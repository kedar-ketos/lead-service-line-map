{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lead Service Line Map Lead Service Line Map is a fast, automated way of generating lead service line maps for utilities. Users can build their own lead service line maps in three easy steps. Structure Demo application : Open Documentation : https://kedar-ketos.github.io/lead-service-line-map Source Code : https://github.com/kedar-ketos/lead-service-line-map These are the only three actions that utilities take to generate their own lead service line map using our approach: Get your datasets Format data Refresh dashboard The need It was developed by KETOS as a submission entry to the Water Data Prize 2021. The first step in building an accurate lead service line inventory is consolidating the existing data and visualizing it on a map. Mapping allows decision-makers to understand the intensity of the challenge posed by lead service lines, prioritize efforts and plan execution. Maps also help keep residents updated about the replacement efforts and adopt healthy drinking water practices. Unfortunately, most utilities do not have the resources to develop a service line mapping solution. Although several reasonable mapping solutions exist ( DC Water , Pittsburgh's PWSA , Flint service line map ), they are often custom-built for specific utilities. In addition, some of them assume specialized geographic information system (GIS) skills and, once adapted, do not allow the user organization flexibility to alter their underlying infrastructure. Finally, some maps bombard the viewers with considerable information taking away focus from the most critical messages. We have addressed these challenges in our solution by combining a model for data storage with a publicly available mapping tool. At the same time, the two parts of our solution are not dependent on one another. This decoupling of the data storage and visualization gives users the flexibility to switch the mapping tool as they see fit. We have made our maps interactive, fun, and enjoyable to read. We achieved this by combining some excellent visual concepts with our visual suggestions. Challenges with existing mapping solutions 1. Generalizability Inability to generalize maps across different utilities is probably the most significant challenge that the existing mapping solutions face today. At best, these maps are typically designed and built with certain large utilities. Some use excellent mapping concepts, but they inadvertently limit their rapid, widespread adoption by not allowing all utilities to replicate these maps for their geographies. 2. Rigid data infrastructure The most widely used dynamic mapping solutions expect the users to convert their data into a format their tools support. As a result, a significant amount of effort goes into preparing data. Furthermore, to ensure that the maps dynamically update, the user organizations might need to rethink their existing data storage and transformation infrastructure. 3. Difficult to read We also observed that viewers are sometimes overwhelmed by the volume of content they see when they visit their lead service line maps. We think users deserve to view all available data about lead lines, but a viewer\u2019s interaction is higher if they are presented that data gradually. Some lead service line maps also tend to be verbose and text-heavy, discouraging viewer engagement. Besides, they also lack proper guidelines about what a user should do next. Although we know that viewers tend to engage more when a data visualization is conversational, we saw that the existing maps are not very engaging. Features of our solution We combined the best practices used in the existing mapping solutions with newer proposals to improve these visuals. Specifically, we tried to address the three challenges outlined above - generalizability, rigidity in infrastructure, and difficulty in reading. Here are the significant features of our solution. We discuss each of these in greater depth in the subsequent sections. Universal data model Decoupled data storage and visualization layers Share information in stages User-friendly design and work-flow Greater engagement with data","title":"Overview"},{"location":"#lead-service-line-map","text":"Lead Service Line Map is a fast, automated way of generating lead service line maps for utilities. Users can build their own lead service line maps in three easy steps.","title":"Lead Service Line Map"},{"location":"#structure","text":"Demo application : Open Documentation : https://kedar-ketos.github.io/lead-service-line-map Source Code : https://github.com/kedar-ketos/lead-service-line-map These are the only three actions that utilities take to generate their own lead service line map using our approach: Get your datasets Format data Refresh dashboard","title":"Structure"},{"location":"#the-need","text":"It was developed by KETOS as a submission entry to the Water Data Prize 2021. The first step in building an accurate lead service line inventory is consolidating the existing data and visualizing it on a map. Mapping allows decision-makers to understand the intensity of the challenge posed by lead service lines, prioritize efforts and plan execution. Maps also help keep residents updated about the replacement efforts and adopt healthy drinking water practices. Unfortunately, most utilities do not have the resources to develop a service line mapping solution. Although several reasonable mapping solutions exist ( DC Water , Pittsburgh's PWSA , Flint service line map ), they are often custom-built for specific utilities. In addition, some of them assume specialized geographic information system (GIS) skills and, once adapted, do not allow the user organization flexibility to alter their underlying infrastructure. Finally, some maps bombard the viewers with considerable information taking away focus from the most critical messages. We have addressed these challenges in our solution by combining a model for data storage with a publicly available mapping tool. At the same time, the two parts of our solution are not dependent on one another. This decoupling of the data storage and visualization gives users the flexibility to switch the mapping tool as they see fit. We have made our maps interactive, fun, and enjoyable to read. We achieved this by combining some excellent visual concepts with our visual suggestions.","title":"The need"},{"location":"#challenges-with-existing-mapping-solutions","text":"","title":"Challenges with existing mapping solutions"},{"location":"#1-generalizability","text":"Inability to generalize maps across different utilities is probably the most significant challenge that the existing mapping solutions face today. At best, these maps are typically designed and built with certain large utilities. Some use excellent mapping concepts, but they inadvertently limit their rapid, widespread adoption by not allowing all utilities to replicate these maps for their geographies.","title":"1. Generalizability"},{"location":"#2-rigid-data-infrastructure","text":"The most widely used dynamic mapping solutions expect the users to convert their data into a format their tools support. As a result, a significant amount of effort goes into preparing data. Furthermore, to ensure that the maps dynamically update, the user organizations might need to rethink their existing data storage and transformation infrastructure.","title":"2. Rigid data infrastructure"},{"location":"#3-difficult-to-read","text":"We also observed that viewers are sometimes overwhelmed by the volume of content they see when they visit their lead service line maps. We think users deserve to view all available data about lead lines, but a viewer\u2019s interaction is higher if they are presented that data gradually. Some lead service line maps also tend to be verbose and text-heavy, discouraging viewer engagement. Besides, they also lack proper guidelines about what a user should do next. Although we know that viewers tend to engage more when a data visualization is conversational, we saw that the existing maps are not very engaging.","title":"3. Difficult to read"},{"location":"#features-of-our-solution","text":"We combined the best practices used in the existing mapping solutions with newer proposals to improve these visuals. Specifically, we tried to address the three challenges outlined above - generalizability, rigidity in infrastructure, and difficulty in reading. Here are the significant features of our solution. We discuss each of these in greater depth in the subsequent sections. Universal data model Decoupled data storage and visualization layers Share information in stages User-friendly design and work-flow Greater engagement with data","title":"Features of our solution"},{"location":"contact/","text":"In case of any issues, questions or feedback, please contact us at kedar.dabhadkar@ketos.co or ganesh@ketos.co . Learn more about KETOS here .","title":"Contact us"},{"location":"demo/data_sources/","text":"Our demo application surfaces a few additional insights apart from service lines. We stitched together different types of datasets in our model to make this possible. Here are the major data sources that we used: Lead service line information from the city of Pittsburgh ( Source ) Pittsburgh tax and property assessment data ( Source ) Pittsburgh neighborhood boundaries ( Source ) Pittsburgh American Community Survey 2014 ( Source ) SDWA violations and LCR drinking water samples data ( Source )","title":"2. Data sources"},{"location":"demo/features/","text":"The data that we generated using the approach outlined in this documentation has been hosted here . Note that if you followed the tutorial as discussed under the tutorial section, then you will be able to build a short version of the deployed demo. That's because when making the demo, we extended the principles of our approach by including additional datasets. That allowed us to have additional stories the the demo. We will try to discuss some of those data sources as well as the features of this approach in this document. Feature 1. The data model Developing a dimensional data model gives us the flexibility to easily replace any dataset while maintaining its relationships with other datasets in the model. As a result of this, we can maintain datasets completely independently of our data visualizations and also refresh our datasets using any tool at our disposal - programming languages, APIs, or even local spreadsheets. The data model that we used for this demo application is showed in the image below. As you can see, we have also included data from SDWA violations to show additional insights. Note Notice how easily we can swap out one table with another one without affecting the other datasets. This is very convenient if the other datasets are huge in size. Also, we can easily swap Pittsburgh data for say, Chicago, as in the tutorial. Feature 2. Share information in stages We observed that one of the challenges in reading the existing mapping solutions for lead service lines is that they overwhelm the user with a lot of information all at once. We tried to solve this by sharing the most relevant first and then allowing users to dive deeper, if they wish to. Feature 3. User-friendly user work-flow We have added animated guides, tutorials, and user-friendly illustrations of service lines wherever relevant. In addition to that, our insights encourage viewers to engage by asking them questions by interacting with the visuals. Feature 4. Higher engagement with data Our solution allows users to download any data that goes into making any visual by selecting the download option at the top right-hand corner of the graphic.","title":"1. Overview & features"},{"location":"demo/features/#feature-1-the-data-model","text":"Developing a dimensional data model gives us the flexibility to easily replace any dataset while maintaining its relationships with other datasets in the model. As a result of this, we can maintain datasets completely independently of our data visualizations and also refresh our datasets using any tool at our disposal - programming languages, APIs, or even local spreadsheets. The data model that we used for this demo application is showed in the image below. As you can see, we have also included data from SDWA violations to show additional insights. Note Notice how easily we can swap out one table with another one without affecting the other datasets. This is very convenient if the other datasets are huge in size. Also, we can easily swap Pittsburgh data for say, Chicago, as in the tutorial.","title":"Feature 1. The data model"},{"location":"demo/features/#feature-2-share-information-in-stages","text":"We observed that one of the challenges in reading the existing mapping solutions for lead service lines is that they overwhelm the user with a lot of information all at once. We tried to solve this by sharing the most relevant first and then allowing users to dive deeper, if they wish to.","title":"Feature 2. Share information in stages"},{"location":"demo/features/#feature-3-user-friendly-user-work-flow","text":"We have added animated guides, tutorials, and user-friendly illustrations of service lines wherever relevant. In addition to that, our insights encourage viewers to engage by asking them questions by interacting with the visuals.","title":"Feature 3. User-friendly user work-flow"},{"location":"demo/features/#feature-4-higher-engagement-with-data","text":"Our solution allows users to download any data that goes into making any visual by selecting the download option at the top right-hand corner of the graphic.","title":"Feature 4. Higher engagement with data"},{"location":"tutorial/get_data/","text":"The first step in map development is to get data that we need followed by transforming it. By the end of this section of the tutorial, we will be able to import the following datasets: GeoJSON neighborhood boundaries for our chosen city factLSL: Dataset containing lead service line meterial dimLSL: Table to provide more context to each service line type. This is a static table and no modification is needed. dimLSLType: A mapper table to bridge the relevant column in factLSL with a matching column in dimLSL. This is a static table and no modification is needed. We random selected the city of Chicago to use in this tutorial. As you will realize later in this tutorial, we could have picked any city that has neighborhood GeoJSON information readily available publicly. 1. Neighborhood GeoJSON data 1.1. Get data Chicago neighborhood boundary dataset is readily available on their data portal . They give the option to download this dataset in a number of different formats. We need the GeoJSON format, which we will download and save in the path Data/Boundaries - Neighborhoods.geojson . The online copy of this dataset is also available in the project repository here . 1.2. Transform data For this tutorial, we will use geojson.io to modify our GeoJSON file. If you navigate to this website and upload the downloaded 'Boundaries - Neighborhoods.geojson' file, this is how you will see the data in a tabular format. Notice how the tool automatically identified neighborhood boundaries. For now, we will only concern ourselves with the pri_neigh column which stands for 'primary neighborhood name'. This is the column that needs to match letter-for-letter to whatever neighborhood names that we choose to use in our main lead service line dataset factLSL . Let's go ahead and rename this column to Neighborhood for consistency. The modified GeoJSON looks like the image below. Go ahead and download this data as GeoJSON. You may overwrite the older file, if you wish to. 2. Service line material information (factLSL.csv) 2.1. Get data To build a real map, the assumption is that the information of what service lines are made of is available (lead or non-lead, both for public and private sides of the line). But in the absence of that, we can generate dummy data from the neighborhood boundary data that we have. Irrespective of wheter we use real or dummy data, we need to generate a table named factLSL.csv and we want it to have the following columns and data types: Address (text): Property street addresses. Latitude (float): Latitude of the address location. Longitude (float): Longitude coordinate of the address location. LSLType (text): Categorization of the material used in the lead service line. You can choose to use any terminology that you prefer in this text column as long as the text indicates material used for both the public and privates sides of service lines. We will see in a later section what we can do to ensure that our tools which is lead and which isn't. Neighborhood : Neighborhood of the address location. For the sake of this tutorial, we generated dummy data for all the columns. The only restriction that we need to follow is ensuring that our latitudes and longitudes fall within the neighborhood boundaries. We used Python to do this, only so that we can generate hundres of dummy data points but you may choose to generate a spreadsheet manually. Feel free to use any tool that you want to for this purpose. The data that we generated looks like this (image below). 3. Map line material to lead or non-lead (dimLSLType.csv) You may choose to use any existing nomenclature to indicate whether each side (public or private) of the service line is made of lead or now. We choose the following nomenclature: Examples: No Data / No Data : No available data either about the public or the private sides of the line. Non-Lead / Lead : Non-lead on the public side but lead on the private side. Lead / Galvanized Iron : Lead on the public side but galvanized iron (non-lead) on the private side. Using the tables dimLSL.csv and dimLSLType.csv , we map our custom nomenclature with a terminology that our tool can understand. We don't need to touch dimLSL.csv at all. Let's go ahead and make make sure that the mapping in dimLSLType.csv is as we expect. For example, We want No Data / No Data to map to \"Public side unknown and private side unknown\". The symbol for this under the column LSLCategory is UU . Similarly, we want Lead / Galvanized Iron to mean \"Public side is lead and private side is non-lead\". The symbol for this is LN . Here are the available symbols and what each of them mean - NN : Non-lead on both sides UU : Unknown material on both sides UL : Unknown material on public side but lead on the private side NL : Non-lead on public side but lead on the private side NU : Non-lead on public side and uknown material on the private side LN : Lead on the public side and non-lead on the private side UN : Unknown material on public side but non-lead on the private side LL : Lead material on both sides LU : Lead on the public but unknown material on the private side If you are using the same terminology as we are, then your dimLSLType.csv will look like this - Summary So far, we prepared all the datasets that we needed for the final mapping tool to work! We should have these files in our Data path: Boundaries - Neighborhoods.geojson factLSL.csv dimLSLType.csv And one dataset that needn't change at all - dimLSL.csv That's all the data transformation there was. We have done all the heavy-lifting so far. In the next section, let's see how we can use these datasets to develop our maps.","title":"2. Get and transform data"},{"location":"tutorial/get_data/#1-neighborhood-geojson-data","text":"","title":"1. Neighborhood GeoJSON data"},{"location":"tutorial/get_data/#11-get-data","text":"Chicago neighborhood boundary dataset is readily available on their data portal . They give the option to download this dataset in a number of different formats. We need the GeoJSON format, which we will download and save in the path Data/Boundaries - Neighborhoods.geojson . The online copy of this dataset is also available in the project repository here .","title":"1.1. Get data"},{"location":"tutorial/get_data/#12-transform-data","text":"For this tutorial, we will use geojson.io to modify our GeoJSON file. If you navigate to this website and upload the downloaded 'Boundaries - Neighborhoods.geojson' file, this is how you will see the data in a tabular format. Notice how the tool automatically identified neighborhood boundaries. For now, we will only concern ourselves with the pri_neigh column which stands for 'primary neighborhood name'. This is the column that needs to match letter-for-letter to whatever neighborhood names that we choose to use in our main lead service line dataset factLSL . Let's go ahead and rename this column to Neighborhood for consistency. The modified GeoJSON looks like the image below. Go ahead and download this data as GeoJSON. You may overwrite the older file, if you wish to.","title":"1.2. Transform data"},{"location":"tutorial/get_data/#2-service-line-material-information-factlslcsv","text":"","title":"2. Service line material information (factLSL.csv)"},{"location":"tutorial/get_data/#21-get-data","text":"To build a real map, the assumption is that the information of what service lines are made of is available (lead or non-lead, both for public and private sides of the line). But in the absence of that, we can generate dummy data from the neighborhood boundary data that we have. Irrespective of wheter we use real or dummy data, we need to generate a table named factLSL.csv and we want it to have the following columns and data types: Address (text): Property street addresses. Latitude (float): Latitude of the address location. Longitude (float): Longitude coordinate of the address location. LSLType (text): Categorization of the material used in the lead service line. You can choose to use any terminology that you prefer in this text column as long as the text indicates material used for both the public and privates sides of service lines. We will see in a later section what we can do to ensure that our tools which is lead and which isn't. Neighborhood : Neighborhood of the address location. For the sake of this tutorial, we generated dummy data for all the columns. The only restriction that we need to follow is ensuring that our latitudes and longitudes fall within the neighborhood boundaries. We used Python to do this, only so that we can generate hundres of dummy data points but you may choose to generate a spreadsheet manually. Feel free to use any tool that you want to for this purpose. The data that we generated looks like this (image below).","title":"2.1. Get data"},{"location":"tutorial/get_data/#3-map-line-material-to-lead-or-non-lead-dimlsltypecsv","text":"You may choose to use any existing nomenclature to indicate whether each side (public or private) of the service line is made of lead or now. We choose the following nomenclature: Examples: No Data / No Data : No available data either about the public or the private sides of the line. Non-Lead / Lead : Non-lead on the public side but lead on the private side. Lead / Galvanized Iron : Lead on the public side but galvanized iron (non-lead) on the private side. Using the tables dimLSL.csv and dimLSLType.csv , we map our custom nomenclature with a terminology that our tool can understand. We don't need to touch dimLSL.csv at all. Let's go ahead and make make sure that the mapping in dimLSLType.csv is as we expect. For example, We want No Data / No Data to map to \"Public side unknown and private side unknown\". The symbol for this under the column LSLCategory is UU . Similarly, we want Lead / Galvanized Iron to mean \"Public side is lead and private side is non-lead\". The symbol for this is LN . Here are the available symbols and what each of them mean - NN : Non-lead on both sides UU : Unknown material on both sides UL : Unknown material on public side but lead on the private side NL : Non-lead on public side but lead on the private side NU : Non-lead on public side and uknown material on the private side LN : Lead on the public side and non-lead on the private side UN : Unknown material on public side but non-lead on the private side LL : Lead material on both sides LU : Lead on the public but unknown material on the private side If you are using the same terminology as we are, then your dimLSLType.csv will look like this -","title":"3. Map line material to lead or non-lead (dimLSLType.csv)"},{"location":"tutorial/get_data/#summary","text":"So far, we prepared all the datasets that we needed for the final mapping tool to work! We should have these files in our Data path: Boundaries - Neighborhoods.geojson factLSL.csv dimLSLType.csv And one dataset that needn't change at all - dimLSL.csv That's all the data transformation there was. We have done all the heavy-lifting so far. In the next section, let's see how we can use these datasets to develop our maps.","title":"Summary"},{"location":"tutorial/objective/","text":"The objective of this tutorial is to develop a map with insights to visualize the status of lead service lines. By the time we finish this tutorial, we will be able to develop a reduced version of this report . The datasets that we use will belong to a single city or a municipality. For the sake of this tutorial, we have chosen the city of Chicago. You may choose your own city or you can use the same datasets that are provided along with this tutorial. Although it's not mandatory to have the actual lead service line dataset, it is highly recommended. In the absence of this dataset, you should have the ability to generate dummy data randomly. We'll discuss in greater depth which dataset columns we need data for. The tutorial is arranged in four sections: Prerequisites: Everything that you need before you start building. Get and transform data: What datasets to get and how to prepare them for visualization. Edit Power BI: Connect your datasets to Power BI and make some minor configuration changes. Publish: Publish your report to Power BI cloud.","title":"Objective"},{"location":"tutorial/power_bi/","text":"By now, we have all the datasets that we need to start developing our Power BI dashboard. Step 1. Open the Power BI file With the GitHub that you downloaded, there's a Power BI file, Lead Service Line Map.pbix . Go ahead and open it and navigate around if you want to get some familiarity. We don't need any prior Power BI knowledge for this tutorial. Step 2. Change data sources Now we need to edit the data source of two of our files factLSL.csv and dimLSLType.csv to point to our spreadsheets. For each of these tables, do the following - Navigate to the Table icon in the left sidebar Click on the table name from the right factLSL.csv or dimLSLType.csv Select the three dots, followed by Edit query . In the new window that pops up, go the right sidebar and select the settings / wheel icon next to Source . Browse to select the relevant spreadsheet. Click Transform and save on the top left menu bar. Step 3. Change GeoJSON data URL First of all, upload the downloaded GeoJSON file to a web server. The best place to upload this file is to your GitHub repository. Get the URL to the file. If you uploaded the file to GitHub, the URL will look something like this https://raw.githubusercontent.com/kedar-ketos/lead-service-line-map/4ebab02d7f6eb1726c4d6e69872df1f74c273488/Data/Boundaries%20-%20Neighborhoods.geojson . Navigate to the Lead Service Line tab in Power BI. Click on the map visual, select the Format brush icon on the right under the Visualizations toolbar. Go down to GeoJSON and paste the URL. Navigate around the Power BI report to checif everything works as expected. We don't have a debug section to this documentation yet, but watch out for that. Summary With these three easy steps, we have now connected the available datasets to Power BI. Next, we will see how to share our Power BI with everyone.","title":"3. Edit Power BI"},{"location":"tutorial/power_bi/#step-1-open-the-power-bi-file","text":"With the GitHub that you downloaded, there's a Power BI file, Lead Service Line Map.pbix . Go ahead and open it and navigate around if you want to get some familiarity. We don't need any prior Power BI knowledge for this tutorial.","title":"Step 1. Open the Power BI file"},{"location":"tutorial/power_bi/#step-2-change-data-sources","text":"Now we need to edit the data source of two of our files factLSL.csv and dimLSLType.csv to point to our spreadsheets. For each of these tables, do the following - Navigate to the Table icon in the left sidebar Click on the table name from the right factLSL.csv or dimLSLType.csv Select the three dots, followed by Edit query . In the new window that pops up, go the right sidebar and select the settings / wheel icon next to Source . Browse to select the relevant spreadsheet. Click Transform and save on the top left menu bar.","title":"Step 2. Change data sources"},{"location":"tutorial/power_bi/#step-3-change-geojson-data-url","text":"First of all, upload the downloaded GeoJSON file to a web server. The best place to upload this file is to your GitHub repository. Get the URL to the file. If you uploaded the file to GitHub, the URL will look something like this https://raw.githubusercontent.com/kedar-ketos/lead-service-line-map/4ebab02d7f6eb1726c4d6e69872df1f74c273488/Data/Boundaries%20-%20Neighborhoods.geojson . Navigate to the Lead Service Line tab in Power BI. Click on the map visual, select the Format brush icon on the right under the Visualizations toolbar. Go down to GeoJSON and paste the URL. Navigate around the Power BI report to checif everything works as expected. We don't have a debug section to this documentation yet, but watch out for that.","title":"Step 3. Change GeoJSON data URL"},{"location":"tutorial/power_bi/#summary","text":"With these three easy steps, we have now connected the available datasets to Power BI. Next, we will see how to share our Power BI with everyone.","title":"Summary"},{"location":"tutorial/prerequisites/","text":"Before we begin, there are a few things that we need to ensure. Clone the GitHub repository We need the files that are needed for the development of this dashbaord to be available locally for the user to edit. The project repository is available here . You can either using git clone command line functionality or download all files from the repository as a compressed folder. After downloading or cloning these files, this is how the directory structure should look like- - lead-service-line-map - Assets - Data - docs - LICENSE - README.md - Lead Service Line Map.pbix We will discuss these dependencies in greater detail in the 'get data' section . Power BI This tool uses Microsoft Power BI for mapping and data visualization. Power BI can be installed from here . Microsoft has some great free tutorials about using Power BI and it's a very widely used tool. However, its major drawback is its incompatibility with Mac OS devices. We recommend users to use a Windows virtual machine to replicate these steps if they use a MacOS device. If you want to publish to the report to the cloud, then you will also need a Microsoft M365 license. You can find more information about this in Power BI's documentation. Note that, you do not need any licenses for this tutorial. GeoJSON editor Data cleaning is a part of this tutorial. To follow this, we'll need access to a GeoJSON editor. Most programming languages make it very easy to modify GeoJSON files (for example, the GeoPandas library for Python ), but users may also choose to do this using free web tools like geojson.io . Spreadsheet tool We will need access to a spredasheet tool that's able to edit comma-separated (CSV) files. For example, Microsoft Excel.","title":"1. Prerequisites"},{"location":"tutorial/prerequisites/#clone-the-github-repository","text":"We need the files that are needed for the development of this dashbaord to be available locally for the user to edit. The project repository is available here . You can either using git clone command line functionality or download all files from the repository as a compressed folder. After downloading or cloning these files, this is how the directory structure should look like- - lead-service-line-map - Assets - Data - docs - LICENSE - README.md - Lead Service Line Map.pbix We will discuss these dependencies in greater detail in the 'get data' section .","title":"Clone the GitHub repository"},{"location":"tutorial/prerequisites/#power-bi","text":"This tool uses Microsoft Power BI for mapping and data visualization. Power BI can be installed from here . Microsoft has some great free tutorials about using Power BI and it's a very widely used tool. However, its major drawback is its incompatibility with Mac OS devices. We recommend users to use a Windows virtual machine to replicate these steps if they use a MacOS device. If you want to publish to the report to the cloud, then you will also need a Microsoft M365 license. You can find more information about this in Power BI's documentation. Note that, you do not need any licenses for this tutorial.","title":"Power BI"},{"location":"tutorial/prerequisites/#geojson-editor","text":"Data cleaning is a part of this tutorial. To follow this, we'll need access to a GeoJSON editor. Most programming languages make it very easy to modify GeoJSON files (for example, the GeoPandas library for Python ), but users may also choose to do this using free web tools like geojson.io .","title":"GeoJSON editor"},{"location":"tutorial/prerequisites/#spreadsheet-tool","text":"We will need access to a spredasheet tool that's able to edit comma-separated (CSV) files. For example, Microsoft Excel.","title":"Spreadsheet tool"},{"location":"tutorial/publish/","text":"We can publish our report to the web with these three easy steps: Refer to Power BI's documentation for a detailed discussion. 1. Publish the report to your workspace From the cover page of the Power BI report, click on Publish at the top (on the menu bar). Log into your Microsoft account, if you haven't already. Select your workspace. This could be any workspace that you want. 2. Publish to web Go to app.powerbi.com and log in if you already have not. Navigate to your report under the proper workspace. Select File > Embed report > Publish to web (public) 3. Embed in a website or application Power BI will show you the URLs that you can use to embed your report in a website or another application. Note Please refer to Power BI documentation to ensure protection of any sensitive data and for additional security measures.","title":"4. Publish"},{"location":"tutorial/publish/#1-publish-the-report-to-your-workspace","text":"From the cover page of the Power BI report, click on Publish at the top (on the menu bar). Log into your Microsoft account, if you haven't already. Select your workspace. This could be any workspace that you want.","title":"1. Publish the report to your workspace"},{"location":"tutorial/publish/#2-publish-to-web","text":"Go to app.powerbi.com and log in if you already have not. Navigate to your report under the proper workspace. Select File > Embed report > Publish to web (public)","title":"2. Publish to web"},{"location":"tutorial/publish/#3-embed-in-a-website-or-application","text":"Power BI will show you the URLs that you can use to embed your report in a website or another application. Note Please refer to Power BI documentation to ensure protection of any sensitive data and for additional security measures.","title":"3. Embed in a website or application"}]}